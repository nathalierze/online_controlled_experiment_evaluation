{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate prediction accuracy in the field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and merge\n",
    "df = pickle.load(open( '../00_data/cleaned/predictions.pkl','rb') )\n",
    "df = df[['ID','UebungsID','UserID','SatzID','prediction','modus','interventiongroup','Minute']]\n",
    "\n",
    "xmlsaetze = pickle.load(open( '../00_data/cleaned/xmlsaetze.pkl','rb') )\n",
    "xmlsaetze['Minute'] = pd.to_datetime(xmlsaetze['Datum']).dt.minute\n",
    "xmlsaetze = xmlsaetze[['ID','UebungsID','UserID','SatzID','Erfolg','Minute','MehrfachFalsch','Testposition']]\n",
    "\n",
    "df_merged = pd.merge(xmlsaetze, df, on=['UserID','UebungsID','SatzID','Minute'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metrics\n",
    "evaluation = df_merged[['Erfolg','prediction']]\n",
    "\n",
    "def test(evaluation):\n",
    "    if(evaluation.Erfolg == True):\n",
    "        if(evaluation.prediction>0.5):\n",
    "            return 1 #true positiv\n",
    "        else:\n",
    "            return 0 # false negativ\n",
    "    elif(evaluation.Erfolg == False):\n",
    "        if(evaluation.prediction<0.5):\n",
    "            return 3 #true negative\n",
    "        else:\n",
    "            return 4 # false positiv\n",
    "    else:\n",
    "        return  0\n",
    "\n",
    "\n",
    "evaluation['correct'] = df_merged.apply (lambda row: test(row), axis=1)\n",
    "evaluation_grouped = evaluation.groupby(['correct'])['Erfolg'].count()\n",
    "evaluation_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate evaluation metrics\n",
    "precision = evaluation_grouped[1] / (evaluation_grouped[1] + evaluation_grouped[4])\n",
    "recall = evaluation_grouped[1] / (evaluation_grouped[1] + evaluation_grouped[0])\n",
    "accuracy = (evaluation_grouped[1]+evaluation_grouped[3])/ (evaluation_grouped[1]+evaluation_grouped[3]+evaluation_grouped[4]+evaluation_grouped[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)\n",
    "print(recall)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions and acutal outcome\n",
    "g=sns.histplot(data=evaluation, x='prediction',hue='Erfolg',multiple=\"stack\")\n",
    "sns.move_legend(g, \"upper center\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
